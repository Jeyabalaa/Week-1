{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE-LSTM Model Training for Predictive Maintenance\n",
    "\n",
    "This notebook implements a Variational Autoencoder (VAE) combined with LSTM for anomaly detection in predictive maintenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. VAE-LSTM Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, sequence_length):\n",
    "        super(VAE_LSTM, self).__init__()\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder LSTM\n",
    "        self.encoder_lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Latent space\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decoder_lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder_output = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        _, (h_n, _) = self.encoder_lstm(x)\n",
    "        h_n = h_n.squeeze(0)  # Remove batch dimension\n",
    "        mu = self.fc_mu(h_n)\n",
    "        log_var = self.fc_var(h_n)\n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        # Repeat latent vector for sequence length\n",
    "        z = z.unsqueeze(1).repeat(1, self.sequence_length, 1)\n",
    "        \n",
    "        # Decode\n",
    "        h = self.decoder_input(z)\n",
    "        output, _ = self.decoder_lstm(h)\n",
    "        reconstruction = self.decoder_output(output)\n",
    "        return reconstruction\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        reconstruction = self.decode(z)\n",
    "        return reconstruction, mu, log_var\n",
    "    \n",
    "    def loss_function(self, reconstruction, x, mu, log_var):\n",
    "        # Reconstruction loss\n",
    "        recon_loss = nn.MSELoss(reduction='sum')(reconstruction, x)\n",
    "        \n",
    "        # KL divergence\n",
    "        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        \n",
    "        return recon_loss + kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.sequence_length = sequence_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx:idx + self.sequence_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae_lstm(model, train_loader, optimizer, device, num_epochs=100):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            reconstruction, mu, log_var = model(batch)\n",
    "            loss = model.loss_function(reconstruction, batch, mu, log_var)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    return train_losses\n",
    "\n",
    "def compute_reconstruction_error(model, data_loader, device):\n",
    "    model.eval()\n",
    "    errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            reconstruction, _, _ = model(batch)\n",
    "            error = torch.mean((reconstruction - batch) ** 2, dim=[1, 2])\n",
    "            errors.extend(error.cpu().numpy())\n",
    "    \n",
    "    return np.array(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Anomaly Detection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_anomaly_threshold(errors, contamination=0.1):\n",
    "    \"\"\"Set anomaly threshold based on training errors\"\"\"\n",
    "    threshold = np.percentile(errors, (1 - contamination) * 100)\n",
    "    return threshold\n",
    "\n",
    "def detect_anomalies(errors, threshold):\n",
    "    \"\"\"Detect anomalies based on reconstruction error threshold\"\"\"\n",
    "    return errors > threshold\n",
    "\n",
    "def evaluate_anomaly_detection(true_labels, predicted_labels):\n",
    "    \"\"\"Evaluate anomaly detection performance\"\"\"\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(true_labels, predicted_labels))\n",
    "    \n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Calculate AUC if we have probability scores\n",
    "    if len(np.unique(true_labels)) == 2:\n",
    "        auc_score = roc_auc_score(true_labels, predicted_labels)\n",
    "        print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "    \n",
    "    return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_loss(train_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('VAE-LSTM Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_reconstruction_errors(errors, threshold=None, anomalies=None):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(errors, bins=50, alpha=0.7, color='blue')\n",
    "    if threshold:\n",
    "        plt.axvline(x=threshold, color='red', linestyle='--', label=f'Threshold: {threshold:.4f}')\n",
    "    plt.xlabel('Reconstruction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Reconstruction Errors')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(errors, alpha=0.7, color='blue', label='Reconstruction Error')\n",
    "    if threshold:\n",
    "        plt.axhline(y=threshold, color='red', linestyle='--', label=f'Threshold: {threshold:.4f}')\n",
    "    if anomalies is not None:\n",
    "        anomaly_indices = np.where(anomalies)[0]\n",
    "        plt.scatter(anomaly_indices, errors[anomaly_indices], color='red', s=10, label='Anomalies')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Reconstruction Error')\n",
    "    plt.title('Reconstruction Errors Over Time')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_anomaly_comparison(original, reconstructed, anomaly_indices, feature_idx=0):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot original vs reconstructed\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(original[:, feature_idx], label='Original', alpha=0.7)\n",
    "    plt.plot(reconstructed[:, feature_idx], label='Reconstructed', alpha=0.7)\n",
    "    plt.scatter(anomaly_indices, original[anomaly_indices, feature_idx], \n",
    "               color='red', s=20, label='Anomalies')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel(f'Feature {feature_idx}')\n",
    "    plt.title('Original vs Reconstructed Data')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot reconstruction error\n",
    "    plt.subplot(2, 1, 2)\n",
    "    error = np.mean((original - reconstructed) ** 2, axis=1)\n",
    "    plt.plot(error, label='Reconstruction Error', color='orange')\n",
    "    plt.scatter(anomaly_indices, error[anomaly_indices], color='red', s=20, label='Anomalies')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title('Reconstruction Error')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae_lstm_pipeline(data, config=None):\n",
    "    \"\"\"Complete VAE-LSTM training pipeline\"\"\"\n",
    "    if config is None:\n",
    "        config = {\n",
    "            'input_dim': data.shape[1],\n",
    "            'hidden_dim': 64,\n",
    "            'latent_dim': 32,\n",
    "            'sequence_length': 50,\n",
    "            'batch_size': 32,\n",
    "            'num_epochs': 100,\n",
    "            'learning_rate': 1e-3,\n",
    "            'contamination': 0.1\n",
    "        }\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = TimeSeriesDataset(data, config['sequence_length'])\n",
    "    train_loader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = VAE_LSTM(\n",
    "        config['input_dim'],\n",
    "        config['hidden_dim'],\n",
    "        config['latent_dim'],\n",
    "        config['sequence_length']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training VAE-LSTM model...\")\n",
    "    train_losses = train_vae_lstm(model, train_loader, optimizer, device, config['num_epochs'])\n",
    "    \n",
    "    # Compute reconstruction errors on training data\n",
    "    print(\"Computing reconstruction errors...\")\n",
    "    train_errors = compute_reconstruction_error(model, train_loader, device)\n",
    "    \n",
    "    # Set anomaly threshold\n",
    "    threshold = set_anomaly_threshold(train_errors, config['contamination'])\n",
    "    print(f\"Anomaly threshold set to: {threshold:.4f}\")\n",
    "    \n",
    "    # Plot training results\n",
    "    plot_training_loss(train_losses)\n",
    "    plot_reconstruction_errors(train_errors, threshold)\n",
    "    \n",
    "    return model, threshold, train_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vae_lstm_model(model, threshold, config, filepath):\n",
    "    \"\"\"Save trained VAE-LSTM model and parameters\"\"\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'threshold': threshold,\n",
    "        'config': config\n",
    "    }, filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_vae_lstm_model(filepath, device):\n",
    "    \"\"\"Load trained VAE-LSTM model and parameters\"\"\"\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    \n",
    "    config = checkpoint['config']\n",
    "    model = VAE_LSTM(\n",
    "        config['input_dim'],\n",
    "        config['hidden_dim'],\n",
    "        config['latent_dim'],\n",
    "        config['sequence_length']\n",
    "    ).to(device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    threshold = checkpoint['threshold']\n",
    "    \n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    return model, threshold, config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (uncomment and modify for your data)\n",
    "# \n",
    "# # Load and preprocess your data\n",
    "# # data = ... # Your preprocessed time series data (numpy array or pandas DataFrame)\n",
    "# \n",
    "# # Define configuration\n",
    "# config = {\n",
    "#     'input_dim': data.shape[1],\n",
    "#     'hidden_dim': 64,\n",
    "#     'latent_dim': 32,\n",
    "#     'sequence_length': 50,\n",
    "#     'batch_size': 32,\n",
    "#     'num_epochs': 100,\n",
    "#     'learning_rate': 1e-3,\n",
    "#     'contamination': 0.1\n",
    "# }\n",
    "# \n",
    "# # Train model\n",
    "# model, threshold, train_errors = train_vae_lstm_pipeline(data, config)\n",
    "# \n",
    "# # Save model\n",
    "# save_vae_lstm_model(model, threshold, config, 'models/vae_lstm_model.pth')\n",
    "# \n",
    "# # For inference on new data:\n",
    "# # test_dataset = TimeSeriesDataset(test_data, config['sequence_length'])\n",
    "# # test_loader = DataLoader(test_dataset, batch_size=config['batch_size'])\n",
    "# # test_errors = compute_reconstruction_error(model, test_loader, device)\n",
    "# # anomalies = detect_anomalies(test_errors, threshold)\n",
    "\n",
    "print(\"VAE-LSTM training functions defined. Ready to use!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
