{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost RUL Regressor Training for Predictive Maintenance\n",
    "\n",
    "This notebook implements XGBoost regression for Remaining Useful Life (RUL) prediction in predictive maintenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation for RUL Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_rul_data(df, sequence_length=30, rul_column=None):\n",
    "    \"\"\"Prepare data for RUL prediction by creating sequences with RUL targets\"\"\"\n",
    "    \n",
    "    if rul_column is None:\n",
    "        # Assume RUL is the last column or create synthetic RUL\n",
    "        if 'RUL' in df.columns:\n",
    "            rul_column = 'RUL'\n",
    "        else:\n",
    "            print(\"No RUL column found. Creating synthetic RUL based on time.\")\n",
    "            df = df.copy()\n",
    "            df['RUL'] = len(df) - np.arange(len(df))\n",
    "            rul_column = 'RUL'\n",
    "    \n",
    "    sequences = []\n",
    "    rul_targets = []\n",
    "    \n",
    "    for i in range(len(df) - sequence_length):\n",
    "        seq = df.iloc[i:i+sequence_length].drop(columns=[rul_column]).values\n",
    "        rul = df.iloc[i+sequence_length][rul_column]\n",
    "        sequences.append(seq.flatten())  # Flatten for XGBoost\n",
    "        rul_targets.append(rul)\n",
    "    \n",
    "    X = np.array(sequences)\n",
    "    y = np.array(rul_targets)\n",
    "    \n",
    "    print(f\"Created {len(X)} sequences with {X.shape[1]} features each\")\n",
    "    print(f\"RUL range: {y.min():.1f} - {y.max():.1f}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def create_rul_features(df, window_sizes=[5, 10, 20]):\n",
    "    \"\"\"Create additional features for RUL prediction\"\"\"\n",
    "    df_featured = df.copy()\n",
    "    \n",
    "    # Rolling statistics\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        for window in window_sizes:\n",
    "            df_featured[f'{col}_rolling_mean_{window}'] = df[col].rolling(window=window).mean()\n",
    "            df_featured[f'{col}_rolling_std_{window}'] = df[col].rolling(window=window).std()\n",
    "            df_featured[f'{col}_rolling_min_{window}'] = df[col].rolling(window=window).min()\n",
    "            df_featured[f'{col}_rolling_max_{window}'] = df[col].rolling(window=window).max()\n",
    "    \n",
    "    # Trend features\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        df_featured[f'{col}_trend'] = df[col].diff()\n",
    "        df_featured[f'{col}_pct_change'] = df[col].pct_change()\n",
    "    \n",
    "    # Fill NaN values\n",
    "    df_featured = df_featured.fillna(method='bfill').fillna(method='ffill')\n",
    "    \n",
    "    print(f\"Created additional features. New shape: {df_featured.shape}\")\n",
    "    return df_featured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGBoost Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_rul(X_train, y_train, config=None):\n",
    "    \"\"\"Train XGBoost model for RUL prediction\"\"\"\n",
    "    if config is None:\n",
    "        config = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eval_metric': 'rmse',\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.1,\n",
    "            'n_estimators': 100,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    \n",
    "    model = xgb.XGBRegressor(**config)\n",
    "    model.fit(X_train, y_train, verbose=True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def hyperparameter_tuning(X_train, y_train, param_grid=None):\n",
    "    \"\"\"Perform hyperparameter tuning for XGBoost\"\"\"\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'subsample': [0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    \n",
    "    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=3,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV score: {-grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rul_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate RUL prediction model\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Model Evaluation Metrics:\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RÂ²: {r2:.4f}\")\n",
    "    \n",
    "    # Calculate percentage errors\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mape': mape,\n",
    "        'predictions': y_pred,\n",
    "        'actual': y_test\n",
    "    }\n",
    "\n",
    "def cross_validate_model(model, X, y, cv=5):\n",
    "    \"\"\"Perform cross-validation\"\"\"\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='neg_mean_squared_error')\n",
    "    rmse_scores = np.sqrt(-scores)\n",
    "    \n",
    "    print(f\"Cross-validation RMSE: {rmse_scores.mean():.4f} (+/- {rmse_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    return rmse_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_vs_actual(y_true, y_pred, title=\"RUL Predictions vs Actual\"):\n",
    "    \"\"\"Plot predicted vs actual RUL values\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(y_true, y_pred, alpha=0.6, color='blue')\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual RUL')\n",
    "    plt.ylabel('Predicted RUL')\n",
    "    plt.title('Predicted vs Actual RUL')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    errors = y_pred - y_true\n",
    "    plt.hist(errors, bins=50, alpha=0.7, color='green')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Prediction Error Distribution')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(y_true[:100], label='Actual', alpha=0.7)\n",
    "    plt.plot(y_pred[:100], label='Predicted', alpha=0.7)\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('RUL')\n",
    "    plt.title('RUL Over Time (First 100 samples)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(np.abs(errors)[:100], color='red', alpha=0.7)\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Absolute Error')\n",
    "    plt.title('Absolute Prediction Error Over Time')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance(model, feature_names=None, top_n=20):\n",
    "    \"\"\"Plot feature importance\"\"\"\n",
    "    if feature_names is None:\n",
    "        feature_names = [f'feature_{i}' for i in range(len(model.feature_importances_))]\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    importance_df.head(top_n).plot.barh(x='feature', y='importance', ax=plt.gca())\n",
    "    plt.title(f'Top {top_n} Feature Importances')\n",
    "    plt.xlabel('Importance')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(len(importance_df)), importance_df['importance'].values)\n",
    "    plt.xlabel('Feature Rank')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.title('Feature Importance Distribution')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SHAP Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_model_with_shap(model, X_train, X_test, feature_names=None, max_evals=1000):\n",
    "    \"\"\"Use SHAP to explain model predictions\"\"\"\n",
    "    try:\n",
    "        # Create SHAP explainer\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        \n",
    "        # Calculate SHAP values for test set (sample for efficiency)\n",
    "        X_sample = X_test[:min(max_evals, len(X_test))]\n",
    "        shap_values = explainer.shap_values(X_sample)\n",
    "        \n",
    "        # Summary plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values, X_sample, feature_names=feature_names, show=False)\n",
    "        plt.title('SHAP Feature Importance Summary')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Waterfall plot for first prediction\n",
    "        if len(X_sample) > 0:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            shap.plots.waterfall(explainer.expected_value, shap_values[0], X_sample[0], \n",
    "                               feature_names=feature_names, show=False)\n",
    "            plt.title('SHAP Waterfall Plot for First Prediction')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return explainer, shap_values\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"SHAP analysis failed: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_rul_pipeline(data, config=None, tune_hyperparams=False):\n",
    "    \"\"\"Complete XGBoost RUL training pipeline\"\"\"\n",
    "    if config is None:\n",
    "        config = {\n",
    "            'sequence_length': 30,\n",
    "            'test_size': 0.2,\n",
    "            'random_state': 42,\n",
    "            'tune_hyperparams': tune_hyperparams\n",
    "        }\n",
    "    \n",
    "    # Prepare data\n",
    "    print(\"Preparing data for RUL prediction...\")\n",
    "    X, y = prepare_rul_data(data, sequence_length=config['sequence_length'])\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=config['test_size'], random_state=config['random_state']\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # Train model\n",
    "    if tune_hyperparams:\n",
    "        print(\"Performing hyperparameter tuning...\")\n",
    "        model = hyperparameter_tuning(X_train, y_train)\n",
    "    else:\n",
    "        print(\"Training XGBoost model...\")\n",
    "        model = train_xgboost_rul(X_train, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"Evaluating model...\")\n",
    "    eval_results = evaluate_rul_model(model, X_test, y_test)\n",
    "    \n",
    "    # Cross-validation\n",
    "    print(\"Performing cross-validation...\")\n",
    "    cv_scores = cross_validate_model(model, X_train, y_train)\n",
    "    \n",
    "    # Visualizations\n",
    "    plot_predictions_vs_actual(y_test, eval_results['predictions'])\n",
    "    \n",
    "    # Feature importance\n",
    "    importance_df = plot_feature_importance(model)\n",
    "    \n",
    "    # SHAP analysis\n",
    "    print(\"Performing SHAP analysis...\")\n",
    "    explainer, shap_values = explain_model_with_shap(model, X_train, X_test)\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'evaluation': eval_results,\n",
    "        'cv_scores': cv_scores,\n",
    "        'feature_importance': importance_df,\n",
    "        'explainer': explainer,\n",
    "        'shap_values': shap_values,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_xgboost_model(model, filepath):\n",
    "    \"\"\"Save trained XGBoost model\"\"\"\n",
    "    model.save_model(filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_xgboost_model(filepath):\n",
    "    \"\"\"Load trained XGBoost model\"\"\"\n",
    "    model = xgb.XGBRegressor()\n",
    "    model.load_model(filepath)\n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (uncomment and modify for your data)\n",
    "# \n",
    "# # Load and preprocess your data\n",
    "# # data = ... # Your preprocessed time series data (pandas DataFrame)\n",
    "# \n",
    "# # Train RUL prediction model\n",
    "# results = train_xgboost_rul_pipeline(data, tune_hyperparams=False)\n",
    "# \n",
    "# # Save model\n",
    "# save_xgboost_model(results['model'], 'models/xgboost_rul_model.json')\n",
    "# \n",
    "# # For inference on new data:\n",
    "# # new_X, _ = prepare_rul_data(new_data, sequence_length=30)\n",
    "# # rul_predictions = results['model'].predict(new_X)\n",
    "\n",
    "print(\"XGBoost RUL training functions defined. Ready to use!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
